---
title: 'LLM inference optimization: Speculative Decoding'
date: 2024-06-10 22:16:00 Z
---

[Speculative decoding paper](https://arxiv.org/pdf/2211.17192)

## Introduction

LLM inference is a serialized process where each new token is generated conditioned on the previously accumulated tokens. This implies that n iterations of LLM inference requires n steps and these steps can’t be parallelized as the input of the current step depends on the output tokens from the previous step. 

### Available compute capacity
The LLM inference process is inherently bottlenecked by the memory due to the auto regressive (generate one token at a time) nature. In simple terms it means that the wallclock time is dominated by data transfers(model weights, kv cache) as opposed to performing the actual matrix multiplies on GPU. 

This implies we can perform additional parallel computations on GPU per memory access without impacting the wallclock time.  If you want to understand this tradeoff further from first principles, please refer to this [fantastic blog by Horace He](https://horace.io/brrr_intro.html) 

### Tokens vary in difficulty
Some tokens are easier to predict for the LLM than other tokens. Eg for code generation maybe curly braces after if statement, generation of stop words, conjunctions and other easier to predict words. In theory, it should be possible for a smaller model to predict those easier tokens and offload some computation from a larger model.

### Speculative decoding to the rescue
Speculative decoding technique exploits the above observations to enable inference speedup. It consists of using a faster, smaller approximate model(M_q) to predict K lookahead tokens in parallel to the main larger, slower baseline model(M_p). 

The name of the technique comes from these lookahead tokens which are speculative in nature i.e. we first verify that the tokens guessed by the draft model are indeed correct. The key observation is that this verification of K lookahead tokens can happen in a single forward pass. Additionally, forward pass for K tokens takes the same amount of wall clock time as a single token as we are memory bound in inference. This enables a speedup since we can potentially fast forward past multiple easy to guess tokens in a single iteration. 

The idea is inspired by [speculative execution](https://en.wikipedia.org/wiki/Speculative_execution#:~:text=Speculative%20execution%20is%20an%20optimization,known%20that%20it%20is%20needed.) which is commonly employed in modern CPU processors where the processor is typically predicting branches speculatively to better overlap computation and memory access. 

## Notation

Token generation step consists of sampling from a probability distribution over the set of all possible tokens in the vocabulary. Normally this part is abstracted from the end user and we observe the generated tokens directly but for this discussion we would be operating on the probability distribution over tokens hence it is important to understand the notation. 

Single step LLM inference: 
1. **`x`**: Input token sequence 
2. **`p(x)`**: Output probability sequence eg output probability over all possible english letters(ignore tokenization here for sake of clarity). This probability distribution represents the probability of next token 
3. **`x ~ p(x)`**: Next token sampled from the above probability distribution
4. **`p(x)`** : Base model probability distribution
5. **`q(x)`** : Draft model probability distribution

## Overview

1. The basic idea is that in the amount of time it takes to generate a single auto-regressive token on the larger model **`M_p`**, we can generate multiple such tokens on the smaller model **`M_q`**(draft model).
2. We then “check” these generated lookahead tokens and only accept them if it matches some criteria.  
4. We fast forward the baseline model on the speculative tokens generated by the draft model i.e. **`{token1}, {token1, token2}, {token1, token2, token3} , …`** etc  and generate the probability distributions. 
5. In the verification step, we take the probabilities output from the base model and compare the corresponding probabilities from draft model. The speculative sampling algorithm then accepts/rejects some of these generated tokens based on some properties of probability distributions(details below). 
6. The algorithm guarantees that sampling tokens from both models are theoretically equivalent - **`x ~ p(x)`** and **`x ~ q(x)`**

The advantage of above procedure is that algorithm enables the model to skip forward a few tokens if the tokens produced by draft model are deemed "good enough". The hope is that in steady state, we are able to get more than 1 tokens generated in the steady state. In the paper they show 2-3x speedup on baseline implementation. 

## Algorithm

Input: In a single speculative decoding iteration, we have the inputs previously generated tokens called prefix tokens, base model and draft model. 

1. We generate tokens **K lookahead speculative tokens** from the draft model **`spec_tokens`**
2. In parallel, consider the k sequences - **`prefix + spec_tokens[:i]`**
3. For each of the sequences, we extract the corresponding probability distribution from the base model by **`prefix + spec_tokens[:i]`**. This corresponds to one inference generation for the base model.


![Screenshot 2024-06-10 at 3.20.15 PM-4daddb.png](/uploads/Screenshot%202024-06-10%20at%203.20.15%E2%80%AFPM-4daddb.png)

Let's break the accept/reject criteria step wise:

1. If **`q(x) <= p(x)`** then we accept the token since the base model is more likely to generate this token and generally the speculative token stream emitted from the draft model is aligned with the base model token stream

2. If **`q(x) > p(x)`** : In this case we roughly want to reject tokens based on deviation/error roughly speaking if **`q(x)`** is only slightly higher than `p(x)` then we should probably accept the token since the error is fairly low. On the other extreme if **`p(x) = 0`** i.e. base model doesn’t emit the token **`x`**,  then we want to reject this token since the spec token stream is misaligned with the base token stream. This can be accomplished if we sample probabilistically **`(q(x)-p(x))/q(x)`**

Pytorch implementation(adapted and simplified from [vllm repo](https://github.com/cadedaniel/vllm-public/blob/853180f8bc5e335b07f0ef7be8079b3e1b7fe0d3/vllm/model_executor/layers/rejection_sampler.py))

```python
    def get_accepted(
            self,
            target_probs: torch.Tensor,  # [batch_size, k, vocab_size]
            draft_probs: torch.Tensor,  # [batch_size, k, vocab_size]
            draft_token_ids: torch.Tensor,  # [batch_size, k]
    ) -> torch.Tensor:
        r"""Create bool matrix over the proposed draft tokens. If
        True, then a token can be accepted, else it should be
        rejected.
        Returns a bool tensor of shape [batch_size, k] specifying which tokens
        are accepted.
        """
        batch_size, k, _ = draft_probs.shape
        # output shape [batch_size, k] gather corresponding probability values 
        # from vocab prob list based on selected draft token ids
        selected_draft_probs = torch.gather(draft_probs, 2, draft_token_ids.unsqueeze(-1)).squeeze(-1)

        # shape [batch_size, k] gather corresponding indices values
        selected_target_probs = torch.gather(target_probs, 2, draft_token_ids.unsqueeze(-1)).squeeze(-1)

        uniform_rand = torch.rand(batch_size,k)
        # p > q case mark as accepted, 
        # for q > p case, sample
        capped_ratio = torch.minimum(
            selected_target_probs / selected_draft_probs,
            torch.full((1, ), 1, device=target_probs.device))
        accepted = uniform_rand < capped_ratio
        return accepted
```

3. We apply the accept/reject criterias discussed in the above section and find the first of the k token which is rejected(or none if all are accepted)

4. From the rejected token, we append back the tokens upto the rejected token to the input prefix tokens for next epoch of speculative decoding. 

5. Additionally, in order to ensure forward progress for each epoch(in case we fail to get any token accepted) we generate one additional sample from the adjusted probability distribution  **p'(x) = \text{norm}(\max(0, p_{n+1}(x) - q_{n+1}(x)))**

```python
 accepted = self._get_accepted(target_probs, draft_probs,
                                      draft_token_ids)
 
 # generate recovered tokens for case where samples get rejected
 difference = target_probs - draft_probs
 # reshape to prepare for sampling
 recovered_probs = difference.reshape(batch_size * k, vocab_size)
 # sample 1 id along the vocab axis
 recovered_token_ids = torch.multinomial(recovered_probs,num_samples=1)
 # reshape to get the original shape back
 recovered_token_ids = recovered_token_ids.reshape(batch_size, k)
```

6. Very roughly speaking, algorithmically we are trying to sample a new frontier of the token space which lies further away from the frontier of the draft token space. This is done by upweighting the probability mass of tokens in vocab space which are more likely to be generated by the base model when compared to the support draft probability distribution.


https://github.com/vllm-project/vllm/pull/2188