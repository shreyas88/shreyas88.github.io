---
title: Comparing paradigms in LLM application development
date: 2024-06-13 23:07:00 Z
---

When it comes to LLM application development design patterns, how do you evaluate various strategies and come up with the best approach for your problem.  Note that some of these techniques are not exclusive and can be used in combination with each other. 


| Paradigm                       | Advantages                                                                                                                                                                                                                                                                                                      | Disadvantages                                                                                 |
|--------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------|
| Prompt engineering             | <ul><li>Give model few shot examples and clear/detailed instructions</li><li>Usually the first technique to try out, great for building problem intuition</li><li>Low engineering cost, fast iteration loops</li><li>Break down the problem down into steps e.g. Chain of thoughts, react framework</li></ul> | <ul><li>Prompt tokens are expensive, feeding more into the context is expensive and may not make sense</li><li>Not effective in learning new output structure, formatting, programming language etc</li></ul> |
|                                |                                                                                                                                                                                                                                                                                                                 |                                                                                                |
| Retrieval augmented Generation aka RAG | <ul><li>Currently the only scalable way to ingest new knowledge from a bunch of documents given a pretrained model</li><li>Getting the context aka "what the model needs to know"</li><li>Interpretability: It's straightforward to implement interpretability since model answers based on the retrieved context</li><li>Decomposing the problem into generation and retrieval can lead to independent iteration</li></ul> | <ul><li>Medium engineering cost i.e. requires tuning</li><li>RAG pipeline can be expensive to maintain</li><li>Multi-step evaluation is needed (generation & retrieval/ranking)</li></ul> |
|                                |                                                                                                                                                                                                                                                                                                                 |                                                                                                |
| Fine tuning                    | <ul><li>Generally specifying how the model needs to behave i.e. domain style, formatting etc</li><li>Effective at teaching model output structure, new programming language, syntax etc</li><li>More efficient during inference since model can follow instructions and style better (reduced number of context tokens)</li><li>Reduced reliance on extensive prompt tuning</li></ul> | <ul><li>Does not teach the model new knowledge, only emphasizes knowledge that already exists in pretraining</li><li>Supervised dataset creation can be expensive</li><li>Infra cost, hyperparameter tuning cost</li></ul> |
|                                |                                                                                                                                                                                                                                                                                                                 |                                                                                                |
| Agents                         | <ul><li>Ability to solve more complex problems requiring long range context and real world interaction</li><li>Planning and breaking down complex tasks into simpler tasks</li><li>Using complementary LLMâ€™s strength e.g. use code generation LLM + general purpose LLM + LLM fine tuned for tool use</li><li>Incorporate multi-turn human feedback into the loop and switch between human dialogue and tool use</li></ul> | <ul><li>Reliability with the current generation of LLM is still an issue</li><li>Context management and pollution is an issue and a hard problem</li></ul> |


References: 
[A Survey of Techniques for Maximizing LLM Performance
](https://www.youtube.com/watch?v=ahnGLM-RC1Y&ab_channel=OpenAI)<br>
[Mastering LLMs: A Conference For Developers & Data Scientists](https://maven.com/parlance-labs/fine-tuning)
